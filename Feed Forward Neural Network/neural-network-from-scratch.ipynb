{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d69ae9b",
   "metadata": {
    "papermill": {
     "duration": 0.003406,
     "end_time": "2025-02-25T17:40:31.960058",
     "exception": false,
     "start_time": "2025-02-25T17:40:31.956652",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Here we will create a simple Neural Network from Scratch.\n",
    "**To get an intuation how Neural Network works under the hood**\n",
    "\n",
    "![A simple Neural Network](https://machinelearningknowledge.ai/wp-content/uploads/2019/10/Backpropagation.gif)\n",
    "\n",
    "![A neural network](https://i.gifer.com/6fyP.gif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25df76a",
   "metadata": {
    "papermill": {
     "duration": 0.002312,
     "end_time": "2025-02-25T17:40:31.965285",
     "exception": false,
     "start_time": "2025-02-25T17:40:31.962973",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feed Forward Neural Network\n",
    "**We can divide the whole Neural Network Learning process into 2 General Parts:**\n",
    "\n",
    "    1. Feed Forward (When it is learning pattern from the given data)\n",
    "    2. Backpropagation (Error Correction)\n",
    "\n",
    "**Now let's focus on Feed Forward Neural Network**    \n",
    "![FFNNs](https://miro.medium.com/v2/1*pO5X2c28F1ysJhwnmPsy3Q.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c1f9f7",
   "metadata": {
    "papermill": {
     "duration": 0.002164,
     "end_time": "2025-02-25T17:40:31.969933",
     "exception": false,
     "start_time": "2025-02-25T17:40:31.967769",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Architecture\n",
    "\n",
    "## 1. Input Layer with 2 Nodes for 2 input features.\n",
    "## 2. Two Hidden Layers -> ReLU activation Function.\n",
    "## 3. One Output Layer -> 1 Node with Sigmoid activation.\n",
    "## 4. We wrote a code for Softmax Activation function also but did not implement it in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bc2cf5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T17:40:31.975966Z",
     "iopub.status.busy": "2025-02-25T17:40:31.975634Z",
     "iopub.status.idle": "2025-02-25T17:40:31.980769Z",
     "shell.execute_reply": "2025-02-25T17:40:31.979850Z"
    },
    "papermill": {
     "duration": 0.009995,
     "end_time": "2025-02-25T17:40:31.982333",
     "exception": false,
     "start_time": "2025-02-25T17:40:31.972338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # One most important note is that we are using numpy\n",
    "np.random.seed(42) # seed = 42 because it gives nice output of feed forward that why:3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cef01f",
   "metadata": {
    "papermill": {
     "duration": 0.00228,
     "end_time": "2025-02-25T17:40:31.987423",
     "exception": false,
     "start_time": "2025-02-25T17:40:31.985143",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Normally, while reading about neural networks, we pass a single sample of data. But in an actual scenario, the model will not work with a single data point; it will work with a batch of data. For example, each batch will contain as many samples as the batch size specifies. It’s definitely not going to be just one sample of data. That’s why, in an actual case, we need to switch our perspective from scalar to matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e938559",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T17:40:31.993456Z",
     "iopub.status.busy": "2025-02-25T17:40:31.993096Z",
     "iopub.status.idle": "2025-02-25T17:40:31.999845Z",
     "shell.execute_reply": "2025-02-25T17:40:31.998882Z"
    },
    "papermill": {
     "duration": 0.012078,
     "end_time": "2025-02-25T17:40:32.001887",
     "exception": false,
     "start_time": "2025-02-25T17:40:31.989809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37454012, 0.95071431],\n",
       "       [0.73199394, 0.59865848],\n",
       "       [0.15601864, 0.15599452],\n",
       "       [0.05808361, 0.86617615]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"input matrix from input layer\"\"\"\n",
    "X = np.random.rand(4,2) # 4 rows-> 4 samples, 2 cols-> 2 input features. [No output layer yet..haha]\n",
    "X # and boom..we've created our input matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5745abb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T17:40:32.009085Z",
     "iopub.status.busy": "2025-02-25T17:40:32.008722Z",
     "iopub.status.idle": "2025-02-25T17:40:32.015443Z",
     "shell.execute_reply": "2025-02-25T17:40:32.014226Z"
    },
    "papermill": {
     "duration": 0.012243,
     "end_time": "2025-02-25T17:40:32.017202",
     "exception": false,
     "start_time": "2025-02-25T17:40:32.004959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " So here we are initializing the weight matrix and bias matrix\n",
      "    using the input layer's num of nodes and current layer's num of nodes\n",
      "now that we have set our:\n",
      "                                1. weight matrix\n",
      "                                2. bias vector,\n",
      "                                3. and we also have our input matrix, \n",
      "       now we are ready for feed forward Action.....yeeeeaaaa!!!\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "\"\"\" Initializaton and Calculation\"\"\"\n",
    "class Dense_layer:\n",
    "    \n",
    "    print(\"\"\" So here we are initializing the weight matrix and bias matrix\n",
    "    using the input layer's num of nodes and current layer's num of nodes\"\"\")\n",
    "    \n",
    "    def __init__(self,input_layer_nodes,current_layer_nodes): # weight and bias matrix initalization \n",
    "        \n",
    "        self.weight = np.random.randn(input_layer_nodes,current_layer_nodes)*0.01 # Weight Matrix\n",
    "        self.bias = np.zeros((1,current_layer_nodes)) # 0 initialization -> Bias Matrix\n",
    "\n",
    "   \n",
    "    print(\"\"\"now that we have set our:\n",
    "                                1. weight matrix\n",
    "                                2. bias vector,\n",
    "                                3. and we also have our input matrix, \n",
    "       now we are ready for feed forward Action.....yeeeeaaaa!!!\n",
    "    \"\"\")\n",
    "    \n",
    "    def feed_forward(self,input_matrix): # we are sending the input matrix (X) here\n",
    "        self.output = np.dot(input_matrix,self.weight)+self.bias # This is the weighted summation + bias\n",
    "                                                                # Z = (input matrix).(weight matrix) + bias matrix\n",
    "        return self.output \n",
    "    \"\"\"\n",
    "        The weighted summation is also Completed. we got our (Z). Yeeeaaaaa!!!\n",
    "        Now we are ready for activation function. We've to put Z inside activation function.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86fbea68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T17:40:32.024215Z",
     "iopub.status.busy": "2025-02-25T17:40:32.023869Z",
     "iopub.status.idle": "2025-02-25T17:40:32.031292Z",
     "shell.execute_reply": "2025-02-25T17:40:32.030251Z"
    },
    "papermill": {
     "duration": 0.01267,
     "end_time": "2025-02-25T17:40:32.032911",
     "exception": false,
     "start_time": "2025-02-25T17:40:32.020241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU activation Function\n",
      "Sigmoid Activation Function\n",
      "Softmax activation\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"ReLU activation Function\"\"\")\n",
    "class Activation_relu:\n",
    "    def feed_forward(self, Z):\n",
    "        self.output = np.maximum(0,Z) # relu be like no negetivity here, only positivity..haha\n",
    "        return self.output\n",
    "\n",
    "print(\"\"\"Sigmoid Activation Function\"\"\") \n",
    "class Activation_sigmoid:\n",
    "    def feed_forward(self,Z):\n",
    "        self.output = 1/(1+np.exp(-Z))\n",
    "        return self.output \n",
    "\n",
    "print(\"Softmax activation\")\n",
    "class Activation_softmax:\n",
    "    def feed_forward(self,z):\n",
    "        exp_mat = np.exp(z- np.max(z,axis=1,keepdims=True)) # helping with overflowing by shriking the values\n",
    "        normalize_mat = exp_mat/np.sum(exp_mat, axis=1,keepdims=True)\n",
    "        self.output = normalize_mat\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b1af51d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T17:40:32.040517Z",
     "iopub.status.busy": "2025-02-25T17:40:32.040202Z",
     "iopub.status.idle": "2025-02-25T17:40:32.063970Z",
     "shell.execute_reply": "2025-02-25T17:40:32.062459Z"
    },
    "papermill": {
     "duration": 0.029255,
     "end_time": "2025-02-25T17:40:32.065693",
     "exception": false,
     "start_time": "2025-02-25T17:40:32.036438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Matrix:\n",
      " [[0.37454012 0.95071431]\n",
      " [0.73199394 0.59865848]\n",
      " [0.15601864 0.15599452]\n",
      " [0.05808361 0.86617615]]\n",
      "\n",
      "Layer 1 Weights:\n",
      " [[ 0.01579213  0.00767435 -0.00469474  0.0054256 ]\n",
      " [-0.00463418 -0.0046573   0.00241962 -0.0191328 ]]\n",
      "\n",
      "Layer 1 Bias:\n",
      " [[0. 0. 0. 0.]]\n",
      "\n",
      "Layer 1 (Z1):\n",
      " [[ 0.00150901 -0.00155341  0.000542   -0.01615772]\n",
      " [ 0.00878545  0.00282945 -0.001988   -0.00748251]\n",
      " [ 0.00174096  0.00047083 -0.00035502 -0.00213812]\n",
      " [-0.00309675 -0.00358829  0.00182313 -0.01625724]]\n",
      "\n",
      "After ReLU (Layer1 Output):\n",
      " [[0.00150901 0.         0.000542   0.        ]\n",
      " [0.00878545 0.00282945 0.         0.        ]\n",
      " [0.00174096 0.00047083 0.         0.        ]\n",
      " [0.         0.         0.00182313 0.        ]]\n",
      "\n",
      "Layer 2 (Z2):\n",
      " [[-3.09506258e-05 -1.61396458e-05]\n",
      " [-1.80199342e-04 -4.05080499e-05]\n",
      " [-3.47988281e-05 -8.30963637e-06]\n",
      " [-1.65544756e-05 -2.57481578e-05]]\n",
      "\n",
      "After ReLU (Layer2 Output):\n",
      " [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "\n",
      "Layer 3 (Z3):\n",
      " [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "Final Output (Sigmoid):\n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Layer and Activation function defined\"\"\"\n",
    "layer1 = Dense_layer(2,4) # input node, num of node in h1, so the moment you made the obj of the dense clss constructor will initial it automatically\n",
    "activation1 = Activation_relu() #define activation\n",
    "\n",
    "layer2 = Dense_layer(4,2)\n",
    "activation2 = Activation_relu()\n",
    "\n",
    "layer3 = Dense_layer(2,1)\n",
    "output_activation = Activation_sigmoid()     \n",
    "\n",
    "# Now the dense connection\n",
    "#Layer 1:\n",
    "Z1 = layer1.feed_forward(X)\n",
    "layer1_output = activation1.feed_forward(Z1)\n",
    "\n",
    "# Layer 2:\n",
    "Z2 = layer2.feed_forward(layer1_output)\n",
    "layer2_output = activation2.feed_forward(Z2)\n",
    "\n",
    "# Output Layer\n",
    "Z3 = layer3.feed_forward(layer2_output)\n",
    "final_output = output_activation.feed_forward(Z3)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Input Matrix:\\n\", X)\n",
    "print(\"\\nLayer 1 Weights:\\n\", layer1.weight)\n",
    "print(\"\\nLayer 1 Bias:\\n\", layer1.bias)\n",
    "\n",
    "\n",
    "print(\"\\nLayer 1 (Z1):\\n\", Z1)\n",
    "print(\"\\nAfter ReLU (Layer1 Output):\\n\", layer1_output)\n",
    "\n",
    "print(\"\\nLayer 2 (Z2):\\n\", Z2)\n",
    "print(\"\\nAfter ReLU (Layer2 Output):\\n\", layer2_output)\n",
    "\n",
    "print(\"\\nLayer 3 (Z3):\\n\", Z3)\n",
    "print(\"\\nFinal Output (Sigmoid):\\n\", final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af2a1f5",
   "metadata": {
    "papermill": {
     "duration": 0.002824,
     "end_time": "2025-02-25T17:40:32.071884",
     "exception": false,
     "start_time": "2025-02-25T17:40:32.069060",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# This is feed forwar, it will change according to problem type and how you want to organized our Architecture."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3.236314,
   "end_time": "2025-02-25T17:40:32.494290",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-25T17:40:29.257976",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
